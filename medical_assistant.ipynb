{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Domain Assistant via LLM Fine-Tuning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a **medical question-answering assistant** by fine-tuning a Large Language Model (LLM) using parameter-efficient techniques. The assistant is designed to help medical students and healthcare learners by providing accurate, concise answers to medical questions.\n",
    "\n",
    "### Domain: Healthcare / Medical Education\n",
    "\n",
    "**Why this domain?**\n",
    "- Healthcare is one of the most impactful applications of AI, where accurate information can directly improve patient outcomes and medical education.\n",
    "- Medical students and healthcare professionals frequently need quick, reliable answers to clinical questions about diseases, drugs, anatomy, and clinical procedures.\n",
    "- A fine-tuned medical LLM can serve as an interactive study aid, reducing the time spent searching through textbooks and references.\n",
    "\n",
    "### Technical Approach\n",
    "- **Base Model**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0` — a compact yet capable generative LLM that fits within Google Colab's free T4 GPU (16 GB VRAM)\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation) via the `peft` library for parameter-efficient training, updating only ~0.5% of total parameters\n",
    "- **Dataset**: `medalpaca/medical_meadow_medical_flashcards` — 33,955 medical Q&A pairs covering a broad range of medical topics\n",
    "- **Evaluation**: Quantitative metrics (BLEU, ROUGE, Perplexity) and qualitative analysis (base vs. fine-tuned comparison)\n",
    "- **Interface**: Gradio chat UI for interactive medical Q&A"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Install Dependencies\n",
    "# ============================================================\n",
    "# All packages needed for data loading, model training, evaluation, and UI\n",
    "\n",
    "!pip install -q transformers peft datasets evaluate bitsandbytes trl gradio rouge-score nltk accelerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# Import Libraries\n# ============================================================\n\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport time\nimport gc\nimport os\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nimport evaluate as hf_evaluate\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Download required NLTK data for tokenization\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\n\nwarnings.filterwarnings('ignore')\n\n# Verify GPU is available (required for training)\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"WARNING: No GPU detected! Training will be extremely slow.\")\n    print(\"Enable GPU via: Runtime -> Change runtime type -> T4 GPU\")\n\nprint(f\"PyTorch Version: {torch.__version__}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Mount Google Drive (for saving/loading trained adapters)\n",
    "# ============================================================\n",
    "# This allows the notebook to be resumed without re-training\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/medical_lora_adapters\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"Google Drive mounted. Adapters will be saved to: {SAVE_DIR}\")\n",
    "except ImportError:\n",
    "    SAVE_DIR = \"./medical_lora_adapters\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"Not running on Colab. Adapters will be saved locally to: {SAVE_DIR}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Dataset Collection & Preprocessing\n",
    "\n",
    "### Dataset: `medalpaca/medical_meadow_medical_flashcards`\n",
    "\n",
    "This dataset from Hugging Face contains **33,955 medical question-answer pairs** derived from medical flashcards. It covers a wide range of medical topics including:\n",
    "- Anatomy & Physiology\n",
    "- Pharmacology & Drug Mechanisms\n",
    "- Pathology & Disease Processes\n",
    "- Clinical Medicine & Diagnostics\n",
    "- Biochemistry & Molecular Biology\n",
    "\n",
    "The dataset is well-suited for fine-tuning a medical Q&A assistant because:\n",
    "1. It contains concise, factual Q&A pairs (ideal for flashcard-style learning)\n",
    "2. Questions are diverse, covering many medical sub-domains\n",
    "3. Answers are typically focused and informative\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "1. **Load** the dataset from Hugging Face\n",
    "2. **Explore** the data: statistics, sample entries, length distributions\n",
    "3. **Clean** the text: remove empty entries, normalize whitespace, filter extreme lengths\n",
    "4. **Format** into the model's chat template for instruction fine-tuning\n",
    "5. **Split** into train/validation/test sets (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Load Dataset from Hugging Face\n",
    "# ============================================================\n",
    "\n",
    "raw_dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(raw_dataset)\n",
    "print(f\"\\nColumn names: {raw_dataset['train'].column_names}\")\n",
    "print(f\"Total examples: {len(raw_dataset['train']):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Explore the Dataset\n",
    "# ============================================================\n",
    "\n",
    "df = raw_dataset[\"train\"].to_pandas()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine the question and answer column names\n",
    "# The dataset may use 'input'/'output' or 'question'/'answer'\n",
    "col_names = df.columns.tolist()\n",
    "print(f\"Columns: {col_names}\")\n",
    "\n",
    "# Identify question and answer columns\n",
    "if \"input\" in col_names:\n",
    "    q_col, a_col = \"input\", \"output\"\n",
    "elif \"question\" in col_names:\n",
    "    q_col, a_col = \"question\", \"answer\"\n",
    "else:\n",
    "    q_col, a_col = col_names[0], col_names[-1]\n",
    "\n",
    "print(f\"\\nQuestion column: '{q_col}'\")\n",
    "print(f\"Answer column: '{a_col}'\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTotal examples: {len(df):,}\")\n",
    "print(f\"Empty questions: {df[q_col].isna().sum() + (df[q_col] == '').sum()}\")\n",
    "print(f\"Empty answers: {df[a_col].isna().sum() + (df[a_col] == '').sum()}\")\n",
    "\n",
    "# Length statistics\n",
    "df[\"q_length\"] = df[q_col].astype(str).str.len()\n",
    "df[\"a_length\"] = df[a_col].astype(str).str.len()\n",
    "\n",
    "print(f\"\\nQuestion length (chars): mean={df['q_length'].mean():.0f}, \"\n",
    "      f\"median={df['q_length'].median():.0f}, \"\n",
    "      f\"min={df['q_length'].min()}, max={df['q_length'].max()}\")\n",
    "print(f\"Answer length (chars):   mean={df['a_length'].mean():.0f}, \"\n",
    "      f\"median={df['a_length'].median():.0f}, \"\n",
    "      f\"min={df['a_length'].min()}, max={df['a_length'].max()}\")\n",
    "\n",
    "# Display sample entries\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ENTRIES\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Q: {df[q_col].iloc[i][:200]}\")\n",
    "    print(f\"A: {df[a_col].iloc[i][:200]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Visualize Length Distributions\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Question length distribution\n",
    "axes[0].hist(df[\"q_length\"], bins=50, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(\"Question Length Distribution (characters)\")\n",
    "axes[0].set_xlabel(\"Length (characters)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].axvline(df[\"q_length\"].median(), color=\"red\", linestyle=\"--\", label=f\"Median: {df['q_length'].median():.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Answer length distribution\n",
    "axes[1].hist(df[\"a_length\"], bins=50, color=\"coral\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_title(\"Answer Length Distribution (characters)\")\n",
    "axes[1].set_xlabel(\"Length (characters)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].axvline(df[\"a_length\"].median(), color=\"red\", linestyle=\"--\", label=f\"Median: {df['a_length'].median():.0f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word count analysis\n",
    "df[\"q_words\"] = df[q_col].astype(str).str.split().str.len()\n",
    "df[\"a_words\"] = df[a_col].astype(str).str.split().str.len()\n",
    "print(f\"Question word count: mean={df['q_words'].mean():.1f}, median={df['q_words'].median():.0f}\")\n",
    "print(f\"Answer word count:   mean={df['a_words'].mean():.1f}, median={df['a_words'].median():.0f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Formatting Steps\n",
    "\n",
    "The preprocessing pipeline applies the following steps:\n",
    "\n",
    "1. **Remove empty/invalid entries**: Drop rows where either the question or answer is missing or empty\n",
    "2. **Normalize whitespace**: Strip leading/trailing whitespace, collapse multiple spaces\n",
    "3. **Filter by length**: Remove entries that are too short (< 10 chars) or too long (> 2000 chars) to ensure quality and fit within the model's context window\n",
    "4. **Subsample**: Select 3,000 high-quality examples to balance training efficiency with model performance (as recommended in the assignment: 1,000-5,000 examples)\n",
    "5. **Format into chat template**: Convert each Q&A pair into TinyLlama's chat format:\n",
    "   ```\n",
    "   <|system|>\n",
    "   You are a helpful medical assistant...</s>\n",
    "   <|user|>\n",
    "   {question}</s>\n",
    "   <|assistant|>\n",
    "   {answer}</s>\n",
    "   ```\n",
    "6. **Split**: 80% training, 10% validation, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Clean, Filter, and Format the Dataset\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize whitespace and strip a text string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # collapse multiple whitespace\n",
    "    return text\n",
    "\n",
    "\n",
    "# Step 1 & 2: Clean and remove empty entries\n",
    "cleaned_data = []\n",
    "for _, row in df.iterrows():\n",
    "    question = clean_text(str(row[q_col]))\n",
    "    answer = clean_text(str(row[a_col]))\n",
    "\n",
    "    # Step 3: Filter by length (remove too short or too long)\n",
    "    if len(question) < 10 or len(answer) < 10:\n",
    "        continue\n",
    "    if len(question) > 2000 or len(answer) > 2000:\n",
    "        continue\n",
    "\n",
    "    cleaned_data.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "print(f\"Examples after cleaning: {len(cleaned_data):,} (removed {len(df) - len(cleaned_data):,})\")\n",
    "\n",
    "# Step 4: Subsample 3,000 examples for training efficiency\n",
    "SAMPLE_SIZE = 3000\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(cleaned_data), size=min(SAMPLE_SIZE, len(cleaned_data)), replace=False)\n",
    "sampled_data = [cleaned_data[i] for i in sorted(indices)]\n",
    "print(f\"Subsampled to {len(sampled_data):,} examples\")\n",
    "\n",
    "# Step 5: Format into TinyLlama chat template\n",
    "SYSTEM_PROMPT = \"You are a helpful medical assistant. Provide accurate, concise answers to medical questions.\"\n",
    "\n",
    "def format_chat(example):\n",
    "    \"\"\"Format a Q&A pair into TinyLlama's chat template.\"\"\"\n",
    "    text = (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{example['question']}</s>\\n\"\n",
    "        f\"<|assistant|>\\n{example['answer']}</s>\"\n",
    "    )\n",
    "    return {\"text\": text, \"question\": example[\"question\"], \"answer\": example[\"answer\"]}\n",
    "\n",
    "formatted_data = [format_chat(ex) for ex in sampled_data]\n",
    "\n",
    "# Step 6: Train/Validation/Test split (80/10/10)\n",
    "np.random.seed(42)\n",
    "n = len(formatted_data)\n",
    "indices = np.random.permutation(n)\n",
    "\n",
    "train_end = int(0.8 * n)\n",
    "val_end = int(0.9 * n)\n",
    "\n",
    "train_data = [formatted_data[i] for i in indices[:train_end]]\n",
    "val_data = [formatted_data[i] for i in indices[train_end:val_end]]\n",
    "test_data = [formatted_data[i] for i in indices[val_end:]]\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train:      {len(train_dataset):,} examples\")\n",
    "print(f\"  Validation: {len(val_dataset):,} examples\")\n",
    "print(f\"  Test:       {len(test_dataset):,} examples\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Verify Formatted Data\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE FORMATTED ENTRIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Training Example {i+1} ---\")\n",
    "    print(train_dataset[i][\"text\"][:500])\n",
    "    print(\"...\")\n",
    "\n",
    "# Check text length distribution after formatting\n",
    "formatted_lengths = [len(ex[\"text\"]) for ex in train_dataset]\n",
    "print(f\"\\nFormatted text length stats:\")\n",
    "print(f\"  Mean: {np.mean(formatted_lengths):.0f} chars\")\n",
    "print(f\"  Median: {np.median(formatted_lengths):.0f} chars\")\n",
    "print(f\"  Max: {np.max(formatted_lengths)} chars\")\n",
    "print(f\"  Min: {np.min(formatted_lengths)} chars\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Model Selection & Fine-Tuning with LoRA\n",
    "\n",
    "### Model Selection: TinyLlama-1.1B-Chat\n",
    "\n",
    "We chose **TinyLlama/TinyLlama-1.1B-Chat-v1.0** for the following reasons:\n",
    "- **Size**: 1.1B parameters fits comfortably in Colab's T4 GPU (16 GB VRAM) when loaded in 4-bit quantization (~700 MB VRAM)\n",
    "- **Architecture**: Based on Llama 2, a proven architecture for instruction-following tasks\n",
    "- **Pre-training**: Trained on 3 trillion tokens, providing strong general language understanding\n",
    "- **Chat-optimized**: The Chat variant is already fine-tuned for instruction-following, making it an excellent starting point for domain-specific fine-tuning\n",
    "\n",
    "### Fine-Tuning Strategy: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA enables parameter-efficient fine-tuning by:\n",
    "1. Freezing the pre-trained model weights\n",
    "2. Injecting small trainable rank-decomposition matrices into attention layers\n",
    "3. Training only these small adapter matrices (~0.5% of total parameters)\n",
    "\n",
    "Combined with **4-bit quantization** via `bitsandbytes`, this allows fine-tuning a 1.1B parameter model on a free Colab T4 GPU.\n",
    "\n",
    "### Hyperparameter Experiments\n",
    "\n",
    "We conduct **3 experiments** varying learning rate, batch size, training epochs, and LoRA rank to find the optimal configuration:\n",
    "\n",
    "| Experiment | Learning Rate | Batch Size | Epochs | LoRA Rank (r) | LoRA Alpha |\n",
    "|-----------|--------------|------------|--------|---------------|------------|\n",
    "| Exp 1     | 2e-4         | 4          | 3      | 16            | 32         |\n",
    "| Exp 2     | 5e-5         | 2          | 3      | 16            | 32         |\n",
    "| Exp 3     | 1e-4         | 4          | 2      | 8             | 16         |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Model & Tokenizer Configuration\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4-bit quantization config to fit model in Colab GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer (shared across all experiments)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer):,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Check token length of our formatted data\n",
    "sample_tokens = tokenizer(train_dataset[0][\"text\"], return_tensors=\"pt\")\n",
    "print(f\"Sample tokenized length: {sample_tokens['input_ids'].shape[1]} tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Training Experiment Function\n",
    "# ============================================================\n",
    "\n",
    "def run_experiment(\n",
    "    experiment_name,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    lora_r,\n",
    "    lora_alpha,\n",
    "    train_data,\n",
    "    val_data,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single fine-tuning experiment with the given hyperparameters.\n",
    "    Returns a dictionary of results including losses, timing, and memory usage.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  EXPERIMENT: {experiment_name}\")\n",
    "    print(f\"  LR={learning_rate}, Batch={batch_size}, Epochs={num_epochs}, \"\n",
    "          f\"LoRA r={lora_r}, alpha={lora_alpha}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # Check if adapter already exists (resume support)\n",
    "    adapter_path = os.path.join(SAVE_DIR, experiment_name)\n",
    "    if os.path.exists(adapter_path) and os.path.exists(os.path.join(adapter_path, \"adapter_config.json\")):\n",
    "        print(f\"Adapter already exists at {adapter_path}. Skipping training.\")\n",
    "        # Load saved results if available\n",
    "        results_path = os.path.join(adapter_path, \"results.json\")\n",
    "        if os.path.exists(results_path):\n",
    "            import json as json_lib\n",
    "            with open(results_path, \"r\") as f:\n",
    "                return json_lib.load(f)\n",
    "        return None\n",
    "\n",
    "    # Record start time and reset GPU memory tracking\n",
    "    start_time = time.time()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Load a fresh base model for this experiment\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # Gradient accumulation to achieve effective batch size of 8\n",
    "    gradient_accumulation = max(1, 8 // batch_size)\n",
    "\n",
    "    output_dir = f\"./results/{experiment_name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=25,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_ratio=0.1,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    # Create SFTTrainer — handles PEFT model creation internally\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Print trainable parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\\n\")\n",
    "\n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Record metrics\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save the LoRA adapter\n",
    "    trainer.model.save_pretrained(adapter_path)\n",
    "    tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "    # Collect results\n",
    "    results = {\n",
    "        \"experiment\": experiment_name,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gradient_accumulation\": gradient_accumulation,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"train_loss\": round(train_result.training_loss, 4),\n",
    "        \"eval_loss\": round(eval_results[\"eval_loss\"], 4),\n",
    "        \"training_time_min\": round(training_time / 60, 1),\n",
    "        \"peak_gpu_memory_gb\": round(peak_memory, 2),\n",
    "    }\n",
    "\n",
    "    # Save results alongside adapter\n",
    "    import json as json_lib\n",
    "    with open(os.path.join(adapter_path, \"results.json\"), \"w\") as f:\n",
    "        json_lib.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n--- Results for {experiment_name} ---\")\n",
    "    print(f\"  Train Loss:       {results['train_loss']:.4f}\")\n",
    "    print(f\"  Eval Loss:        {results['eval_loss']:.4f}\")\n",
    "    print(f\"  Training Time:    {results['training_time_min']:.1f} min\")\n",
    "    print(f\"  Peak GPU Memory:  {results['peak_gpu_memory_gb']:.2f} GB\")\n",
    "\n",
    "    # Cleanup to free GPU memory for next experiment\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiments\n",
    "\n",
    "We now run three experiments with different hyperparameter configurations. Between each experiment, the model is fully unloaded from GPU memory and reloaded fresh to ensure fair comparison.\n",
    "\n",
    "Each experiment's LoRA adapter is saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Experiment 1: Higher LR, larger batch, 3 epochs, LoRA r=16\n",
    "# ============================================================\n",
    "\n",
    "exp1_results = run_experiment(\n",
    "    experiment_name=\"exp1_lr2e4_bs4_ep3_r16\",\n",
    "    learning_rate=2e-4,\n",
    "    batch_size=4,\n",
    "    num_epochs=3,\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    train_data=train_dataset,\n",
    "    val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Experiment 2: Lower LR, smaller batch, 3 epochs, LoRA r=16\n",
    "# ============================================================\n",
    "\n",
    "exp2_results = run_experiment(\n",
    "    experiment_name=\"exp2_lr5e5_bs2_ep3_r16\",\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=2,\n",
    "    num_epochs=3,\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    train_data=train_dataset,\n",
    "    val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Experiment 3: Medium LR, larger batch, 2 epochs, LoRA r=8\n",
    "# ============================================================\n",
    "\n",
    "exp3_results = run_experiment(\n",
    "    experiment_name=\"exp3_lr1e4_bs4_ep2_r8\",\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=4,\n",
    "    num_epochs=2,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    train_data=train_dataset,\n",
    "    val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Experiment Comparison Table\n",
    "# ============================================================\n",
    "\n",
    "experiment_results = [r for r in [exp1_results, exp2_results, exp3_results] if r is not None]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "display_cols = [\n",
    "    \"experiment\", \"learning_rate\", \"batch_size\", \"epochs\",\n",
    "    \"lora_r\", \"lora_alpha\", \"train_loss\", \"eval_loss\",\n",
    "    \"training_time_min\", \"peak_gpu_memory_gb\"\n",
    "]\n",
    "comparison_df = comparison_df[display_cols]\n",
    "\n",
    "# Rename columns for display\n",
    "comparison_df.columns = [\n",
    "    \"Experiment\", \"Learning Rate\", \"Batch Size\", \"Epochs\",\n",
    "    \"LoRA Rank\", \"LoRA Alpha\", \"Train Loss\", \"Eval Loss\",\n",
    "    \"Time (min)\", \"GPU Mem (GB)\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"EXPERIMENT COMPARISON TABLE\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best experiment (lowest eval loss)\n",
    "best_idx = comparison_df[\"Eval Loss\"].idxmin()\n",
    "best_experiment = experiment_results[best_idx][\"experiment\"]\n",
    "print(f\"\\nBest experiment: {best_experiment} (Eval Loss: {comparison_df.loc[best_idx, 'Eval Loss']:.4f})\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "labels = [r[\"experiment\"].split(\"_\")[0] + \"\\n\" + r[\"experiment\"].split(\"_\")[0] for r in experiment_results]\n",
    "short_labels = [f\"Exp {i+1}\" for i in range(len(experiment_results))]\n",
    "\n",
    "# Training Loss\n",
    "axes[0].bar(short_labels, comparison_df[\"Train Loss\"], color=[\"steelblue\", \"coral\", \"mediumseagreen\"])\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "# Eval Loss\n",
    "axes[1].bar(short_labels, comparison_df[\"Eval Loss\"], color=[\"steelblue\", \"coral\", \"mediumseagreen\"])\n",
    "axes[1].set_title(\"Validation Loss\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "\n",
    "# Training Time\n",
    "axes[2].bar(short_labels, comparison_df[\"Time (min)\"], color=[\"steelblue\", \"coral\", \"mediumseagreen\"])\n",
    "axes[2].set_title(\"Training Time\")\n",
    "axes[2].set_ylabel(\"Minutes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Evaluation & Performance Metrics\n",
    "\n",
    "We evaluate the best-performing model using:\n",
    "\n",
    "### Quantitative Metrics\n",
    "1. **BLEU Score**: Measures n-gram overlap between generated and reference answers (precision-oriented)\n",
    "2. **ROUGE Score**: Measures recall-oriented overlap (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "3. **Perplexity**: Measures how well the model predicts the test data (lower = better)\n",
    "\n",
    "### Qualitative Evaluation\n",
    "4. **Base vs. Fine-tuned Comparison**: Side-by-side responses to demonstrate the value of fine-tuning\n",
    "5. **Out-of-domain Testing**: Verify the model handles non-medical queries appropriately"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Load the Best Model for Evaluation\n",
    "# ============================================================\n",
    "\n",
    "# Load base model\n",
    "best_adapter_path = os.path.join(SAVE_DIR, best_experiment)\n",
    "print(f\"Loading best adapter from: {best_adapter_path}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter on top\n",
    "model = PeftModel.from_pretrained(model, best_adapter_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded successfully.\")\n",
    "print(f\"Model device: {model.device}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Response Generation Helper\n",
    "# ============================================================\n",
    "\n",
    "def generate_response(model, tokenizer, question, max_new_tokens=256):\n",
    "    \"\"\"Generate a response from the model given a medical question.\"\"\"\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{question}</s>\\n\"\n",
    "        f\"<|assistant|>\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated tokens (exclude the prompt)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "# Quick test\n",
    "test_q = \"What are the symptoms of diabetes?\"\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {generate_response(model, tokenizer, test_q)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Compute BLEU & ROUGE on Test Set\n",
    "# ============================================================\n",
    "\n",
    "# Use a subset for evaluation (generation is slow)\n",
    "NUM_EVAL = min(100, len(test_dataset))\n",
    "eval_subset = test_dataset.select(range(NUM_EVAL))\n",
    "\n",
    "print(f\"Evaluating on {NUM_EVAL} test examples...\")\n",
    "print(\"Generating responses (this may take a few minutes)...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, example in enumerate(eval_subset):\n",
    "    # Generate prediction\n",
    "    pred = generate_response(model, tokenizer, example[\"question\"], max_new_tokens=200)\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"answer\"])\n",
    "\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Generated {i + 1}/{NUM_EVAL} responses...\")\n",
    "\n",
    "print(f\"  Done! Generated {NUM_EVAL} responses.\")\n",
    "\n",
    "# --- BLEU Score ---\n",
    "smoothie = SmoothingFunction().method1\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, pred in zip(references, predictions):\n",
    "    ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "    pred_tokens = nltk.word_tokenize(pred.lower())\n",
    "\n",
    "    if len(pred_tokens) == 0:\n",
    "        bleu_scores.append(0.0)\n",
    "        continue\n",
    "\n",
    "    score = sentence_bleu(\n",
    "        [ref_tokens],\n",
    "        pred_tokens,\n",
    "        smoothing_function=smoothie,\n",
    "    )\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\"\\nBLEU Score: {avg_bleu:.4f}\")\n",
    "\n",
    "# --- ROUGE Score ---\n",
    "rouge = hf_evaluate.load(\"rouge\")\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-1:   {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2:   {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L:   {rouge_results['rougeL']:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Compute Perplexity on Test Set\n",
    "# ============================================================\n",
    "\n",
    "print(\"Computing perplexity on test set...\")\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "# Use formatted test texts (full Q&A pairs)\n",
    "for i, example in enumerate(eval_subset):\n",
    "    inputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=inputs[\"input_ids\"],\n",
    "        )\n",
    "\n",
    "    # Accumulate loss weighted by number of tokens\n",
    "    num_tokens = inputs[\"attention_mask\"].sum().item()\n",
    "    total_loss += outputs.loss.item() * num_tokens\n",
    "    total_tokens += num_tokens\n",
    "\n",
    "avg_loss = total_loss / total_tokens\n",
    "perplexity = float(np.exp(avg_loss))\n",
    "\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity:   {perplexity:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Evaluation Metrics Summary Table\n",
    "# ============================================================\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Metric\": [\"BLEU Score\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"Perplexity\"],\n",
    "    \"Value\": [\n",
    "        f\"{avg_bleu:.4f}\",\n",
    "        f\"{rouge_results['rouge1']:.4f}\",\n",
    "        f\"{rouge_results['rouge2']:.4f}\",\n",
    "        f\"{rouge_results['rougeL']:.4f}\",\n",
    "        f\"{perplexity:.2f}\",\n",
    "    ],\n",
    "    \"Interpretation\": [\n",
    "        \"Higher is better (0-1 scale, measures n-gram precision)\",\n",
    "        \"Higher is better (unigram recall overlap)\",\n",
    "        \"Higher is better (bigram recall overlap)\",\n",
    "        \"Higher is better (longest common subsequence)\",\n",
    "        \"Lower is better (model confidence on test data)\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_df.to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation: Base Model vs. Fine-Tuned Model\n",
    "\n",
    "Below we compare responses from the **base TinyLlama model** (without fine-tuning) against our **fine-tuned medical assistant** on the same set of medical questions. This demonstrates the concrete value that domain-specific fine-tuning provides."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Base vs. Fine-Tuned Model Comparison\n",
    "# ============================================================\n",
    "\n",
    "# Test questions covering different medical topics\n",
    "comparison_questions = [\n",
    "    \"What are the main symptoms of Type 2 diabetes?\",\n",
    "    \"Explain the mechanism of action of aspirin.\",\n",
    "    \"What is the difference between benign and malignant tumors?\",\n",
    "    \"What are the risk factors for cardiovascular disease?\",\n",
    "    \"Describe the function of the liver in the human body.\",\n",
    "    \"What is hypertension and how is it treated?\",\n",
    "    \"What are the stages of wound healing?\",\n",
    "    \"Explain what an ECG measures and why it is useful.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASE MODEL vs. FINE-TUNED MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, question in enumerate(comparison_questions):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "\n",
    "    # Fine-tuned model response\n",
    "    finetuned_response = generate_response(model, tokenizer, question)\n",
    "\n",
    "    # Base model response (temporarily disable LoRA adapters)\n",
    "    with model.disable_adapter():\n",
    "        base_response = generate_response(model, tokenizer, question)\n",
    "\n",
    "    print(f\"\\n  BASE MODEL:\")\n",
    "    print(f\"  {base_response[:300]}\")\n",
    "    print(f\"\\n  FINE-TUNED MODEL:\")\n",
    "    print(f\"  {finetuned_response[:300]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Out-of-Domain Testing\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OUT-OF-DOMAIN QUERY TESTING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Testing how the model handles non-medical questions:\\n\")\n",
    "\n",
    "ood_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I write a Python for loop?\",\n",
    "    \"What is the recipe for chocolate cake?\",\n",
    "    \"Who won the 2022 FIFA World Cup?\",\n",
    "]\n",
    "\n",
    "for question in ood_questions:\n",
    "    response = generate_response(model, tokenizer, question, max_new_tokens=150)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response[:250]}\")\n",
    "    print()\n",
    "\n",
    "print(\"Note: The model may attempt to answer these questions, but the quality\")\n",
    "print(\"of responses should be noticeably lower for non-medical topics compared\")\n",
    "print(\"to medical questions, demonstrating domain specialization.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Interactive UI with Gradio\n",
    "\n",
    "We deploy the fine-tuned model using **Gradio**, which provides a simple, intuitive chat interface. The interface allows users to:\n",
    "- Type medical questions in natural language\n",
    "- Receive AI-generated responses from our fine-tuned model\n",
    "- Try pre-loaded example questions\n",
    "\n",
    "Using `share=True` creates a public URL that works within Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Gradio Chat Interface\n",
    "# ============================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"Generate a response from the fine-tuned medical assistant.\"\"\"\n",
    "    if not message.strip():\n",
    "        return \"Please enter a medical question.\"\n",
    "\n",
    "    response = generate_response(model, tokenizer, message, max_new_tokens=300)\n",
    "\n",
    "    if not response:\n",
    "        return \"I'm sorry, I couldn't generate a response. Please try rephrasing your question.\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Build the chat interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Medical Assistant - Powered by Fine-Tuned TinyLlama\",\n",
    "    description=(\n",
    "        \"Ask medical questions and receive AI-generated answers. \"\n",
    "        \"This model was fine-tuned on medical flashcard data using LoRA. \"\n",
    "        \"**Disclaimer**: This is an educational tool and should NOT be used for actual medical diagnosis or treatment.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"Explain the mechanism of action of aspirin.\",\n",
    "        \"What is the difference between Type 1 and Type 2 diabetes?\",\n",
    "        \"What are the risk factors for cardiovascular disease?\",\n",
    "        \"Describe the stages of wound healing.\",\n",
    "        \"What is hypertension and how is it treated?\",\n",
    "        \"What does the liver do in the human body?\",\n",
    "    ],\n",
    "    theme=\"soft\",\n",
    ")\n",
    "\n",
    "# Launch with share=True for Colab compatibility\n",
    "demo.launch(share=True, debug=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this project, we built a **medical question-answering assistant** by fine-tuning TinyLlama-1.1B-Chat using LoRA (Low-Rank Adaptation). Key achievements:\n",
    "\n",
    "1. **Dataset**: Preprocessed and curated 3,000 high-quality medical Q&A pairs from the `medalpaca/medical_meadow_medical_flashcards` dataset\n",
    "2. **Efficient Fine-Tuning**: Used 4-bit quantization + LoRA to fine-tune a 1.1B parameter model on a free Google Colab T4 GPU, training only ~0.5% of total parameters\n",
    "3. **Systematic Experimentation**: Compared 3 different hyperparameter configurations, tracking training loss, validation loss, GPU memory, and training time\n",
    "4. **Comprehensive Evaluation**: Measured performance using BLEU, ROUGE, and perplexity metrics, plus qualitative side-by-side comparison between base and fine-tuned models\n",
    "5. **Interactive Deployment**: Built a Gradio chat interface for real-time medical Q&A\n",
    "\n",
    "### Key Findings\n",
    "- **Fine-tuning significantly improves** the model's ability to provide focused, medically relevant answers compared to the base model\n",
    "- **LoRA is highly effective** for domain adaptation — training only a small fraction of parameters yields meaningful improvements\n",
    "- **4-bit quantization** enables training on consumer-grade GPUs without significant quality loss\n",
    "\n",
    "### Limitations\n",
    "- **Model size**: TinyLlama (1.1B params) has inherent limitations in reasoning depth compared to larger models\n",
    "- **Dataset scope**: Medical flashcards cover broad topics but may lack depth in specialized sub-domains\n",
    "- **No clinical validation**: Responses have not been verified by medical professionals\n",
    "- **Hallucination risk**: The model may generate plausible-sounding but incorrect medical information\n",
    "\n",
    "### Future Work\n",
    "- Fine-tune on larger, clinically validated datasets (e.g., PubMedQA, MedMCQA)\n",
    "- Experiment with larger base models (e.g., Gemma 2B, Llama 3.2 3B) if GPU resources allow\n",
    "- Implement retrieval-augmented generation (RAG) for more accurate, source-grounded answers\n",
    "- Add confidence scoring to flag uncertain responses\n",
    "- Conduct evaluation with medical domain experts\n",
    "\n",
    "### Disclaimer\n",
    "This model is an **educational demonstration** and should **NOT** be used for actual medical diagnosis, treatment, or clinical decision-making. Always consult qualified healthcare professionals for medical advice."
   ]
  }
 ]
}