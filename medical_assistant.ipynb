{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Domain Assistant via LLM Fine-Tuning\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a **medical question-answering assistant** by fine-tuning a Large Language Model (LLM) using parameter-efficient techniques. The assistant is designed to help medical students and healthcare learners by providing accurate, concise answers to medical questions.\n",
    "\n",
    "### Domain: Healthcare / Medical Education\n",
    "\n",
    "**Why this domain?**\n",
    "- Healthcare is one of the most impactful applications of AI, where accurate information can directly improve patient outcomes and medical education.\n",
    "- Medical students and healthcare professionals frequently need quick, reliable answers to clinical questions about diseases, drugs, anatomy, and clinical procedures.\n",
    "- A fine-tuned medical LLM can serve as an interactive study aid, reducing the time spent searching through textbooks and references.\n",
    "\n",
    "### Technical Approach\n",
    "- **Base Model**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0` — fits within Google Colab's free T4 GPU (16 GB VRAM)\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation) via the `peft` library for parameter-efficient training\n",
    "- **Dataset**: `medalpaca/medical_meadow_medical_flashcards` — 33,955 medical Q&A pairs\n",
    "- **Evaluation**: BLEU, ROUGE, Perplexity + qualitative analysis (base vs. fine-tuned comparison)\n",
    "- **Interface**: Gradio chat UI for interactive medical Q&A"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "!pip install -q transformers peft datasets evaluate bitsandbytes trl gradio rouge-score nltk accelerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import Libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport time\nimport gc\nimport os\nimport re\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nimport evaluate as hf_evaluate\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nnltk.download('punkt', quiet=True)\nnltk.download('punkt_tab', quiet=True)\nwarnings.filterwarnings('ignore')\n\n# Verify GPU availability\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"WARNING: No GPU detected! Enable GPU via: Runtime -> Change runtime type -> T4 GPU\")\n\nprint(f\"PyTorch Version: {torch.__version__}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Dataset Collection & Preprocessing\n",
    "\n",
    "### Dataset: `medalpaca/medical_meadow_medical_flashcards`\n",
    "\n",
    "This dataset contains **33,955 medical question-answer pairs** derived from medical flashcards covering anatomy, pharmacology, pathology, clinical medicine, and biochemistry.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "1. **Load** the dataset from Hugging Face\n",
    "2. **Explore** the data: statistics, sample entries, length distributions\n",
    "3. **Clean** the text: remove empty entries, normalize whitespace, filter extreme lengths\n",
    "4. **Format** into the model's chat template for instruction fine-tuning\n",
    "5. **Split** into train/validation/test sets (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Dataset from Hugging Face\n",
    "raw_dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(raw_dataset)\n",
    "print(f\"\\nColumn names: {raw_dataset['train'].column_names}\")\n",
    "print(f\"Total examples: {len(raw_dataset['train']):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore the Dataset\n",
    "df = raw_dataset[\"train\"].to_pandas()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "col_names = df.columns.tolist()\n",
    "print(f\"Columns: {col_names}\")\n",
    "\n",
    "# Identify question and answer columns\n",
    "if \"input\" in col_names:\n",
    "    q_col, a_col = \"input\", \"output\"\n",
    "elif \"question\" in col_names:\n",
    "    q_col, a_col = \"question\", \"answer\"\n",
    "else:\n",
    "    q_col, a_col = col_names[0], col_names[-1]\n",
    "\n",
    "print(f\"\\nQuestion column: '{q_col}'\")\n",
    "print(f\"Answer column: '{a_col}'\")\n",
    "print(f\"\\nTotal examples: {len(df):,}\")\n",
    "print(f\"Empty questions: {df[q_col].isna().sum() + (df[q_col] == '').sum()}\")\n",
    "print(f\"Empty answers: {df[a_col].isna().sum() + (df[a_col] == '').sum()}\")\n",
    "\n",
    "# Length statistics\n",
    "df[\"q_length\"] = df[q_col].astype(str).str.len()\n",
    "df[\"a_length\"] = df[a_col].astype(str).str.len()\n",
    "\n",
    "print(f\"\\nQuestion length (chars): mean={df['q_length'].mean():.0f}, \"\n",
    "      f\"median={df['q_length'].median():.0f}, \"\n",
    "      f\"min={df['q_length'].min()}, max={df['q_length'].max()}\")\n",
    "print(f\"Answer length (chars):   mean={df['a_length'].mean():.0f}, \"\n",
    "      f\"median={df['a_length'].median():.0f}, \"\n",
    "      f\"min={df['a_length'].min()}, max={df['a_length'].max()}\")\n",
    "\n",
    "# Display sample entries\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ENTRIES\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Q: {df[q_col].iloc[i][:200]}\")\n",
    "    print(f\"A: {df[a_col].iloc[i][:200]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize Length Distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df[\"q_length\"], bins=50, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(\"Question Length Distribution (characters)\")\n",
    "axes[0].set_xlabel(\"Length (characters)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].axvline(df[\"q_length\"].median(), color=\"red\", linestyle=\"--\", label=f\"Median: {df['q_length'].median():.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df[\"a_length\"], bins=50, color=\"coral\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_title(\"Answer Length Distribution (characters)\")\n",
    "axes[1].set_xlabel(\"Length (characters)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].axvline(df[\"a_length\"].median(), color=\"red\", linestyle=\"--\", label=f\"Median: {df['a_length'].median():.0f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Formatting Steps\n",
    "\n",
    "1. **Remove empty/invalid entries**: Drop rows where question or answer is missing\n",
    "2. **Normalize whitespace**: Strip leading/trailing whitespace, collapse multiple spaces\n",
    "3. **Filter by length**: Remove entries shorter than 10 chars or longer than 2000 chars\n",
    "4. **Subsample**: Select 3,000 high-quality examples (recommended: 1,000-5,000)\n",
    "5. **Format into chat template**: Convert each Q&A pair into TinyLlama's chat format\n",
    "6. **Split**: 80% training, 10% validation, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean, Filter, and Format the Dataset\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize whitespace and strip a text string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Clean and remove empty entries\n",
    "cleaned_data = []\n",
    "for _, row in df.iterrows():\n",
    "    question = clean_text(str(row[q_col]))\n",
    "    answer = clean_text(str(row[a_col]))\n",
    "    if len(question) < 10 or len(answer) < 10:\n",
    "        continue\n",
    "    if len(question) > 2000 or len(answer) > 2000:\n",
    "        continue\n",
    "    cleaned_data.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "print(f\"Examples after cleaning: {len(cleaned_data):,} (removed {len(df) - len(cleaned_data):,})\")\n",
    "\n",
    "# Subsample 3,000 examples for training efficiency\n",
    "SAMPLE_SIZE = 3000\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(cleaned_data), size=min(SAMPLE_SIZE, len(cleaned_data)), replace=False)\n",
    "sampled_data = [cleaned_data[i] for i in sorted(indices)]\n",
    "print(f\"Subsampled to {len(sampled_data):,} examples\")\n",
    "\n",
    "# Format into TinyLlama chat template\n",
    "SYSTEM_PROMPT = \"You are a helpful medical assistant. Provide accurate, concise answers to medical questions.\"\n",
    "\n",
    "def format_chat(example):\n",
    "    \"\"\"Format a Q&A pair into TinyLlama's chat template.\"\"\"\n",
    "    text = (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{example['question']}</s>\\n\"\n",
    "        f\"<|assistant|>\\n{example['answer']}</s>\"\n",
    "    )\n",
    "    return {\"text\": text, \"question\": example[\"question\"], \"answer\": example[\"answer\"]}\n",
    "\n",
    "formatted_data = [format_chat(ex) for ex in sampled_data]\n",
    "\n",
    "# Train/Validation/Test split (80/10/10)\n",
    "np.random.seed(42)\n",
    "n = len(formatted_data)\n",
    "indices = np.random.permutation(n)\n",
    "train_end = int(0.8 * n)\n",
    "val_end = int(0.9 * n)\n",
    "\n",
    "train_dataset = Dataset.from_list([formatted_data[i] for i in indices[:train_end]])\n",
    "val_dataset = Dataset.from_list([formatted_data[i] for i in indices[train_end:val_end]])\n",
    "test_dataset = Dataset.from_list([formatted_data[i] for i in indices[val_end:]])\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train:      {len(train_dataset):,} examples\")\n",
    "print(f\"  Validation: {len(val_dataset):,} examples\")\n",
    "print(f\"  Test:       {len(test_dataset):,} examples\")\n",
    "\n",
    "# Show sample formatted entry\n",
    "print(f\"\\nSample formatted entry:\\n{train_dataset[0]['text'][:400]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Model Selection & Fine-Tuning with LoRA\n",
    "\n",
    "### Model: TinyLlama-1.1B-Chat\n",
    "- 1.1B parameters, fits in Colab T4 GPU with 4-bit quantization\n",
    "- Based on Llama 2 architecture, trained on 3 trillion tokens\n",
    "- Chat-optimized variant for instruction-following\n",
    "\n",
    "### Fine-Tuning: LoRA (Low-Rank Adaptation)\n",
    "- Freezes pre-trained weights, trains small adapter matrices (~0.5% of parameters)\n",
    "- Combined with 4-bit quantization via `bitsandbytes`\n",
    "\n",
    "### Experiments\n",
    "\n",
    "| Experiment | Learning Rate | Batch Size | Epochs | LoRA Rank (r) | LoRA Alpha |\n",
    "|-----------|--------------|------------|--------|---------------|------------|\n",
    "| Exp 1     | 2e-4         | 4          | 3      | 16            | 32         |\n",
    "| Exp 2     | 5e-5         | 2          | 3      | 16            | 32         |\n",
    "| Exp 3     | 1e-4         | 4          | 2      | 8             | 16         |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model & Tokenizer Configuration\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "SAVE_DIR = \"./medical_lora_adapters\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "print(f\"Sample tokenized length: {tokenizer(train_dataset[0]['text'], return_tensors='pt')['input_ids'].shape[1]} tokens\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Training Experiment Function\ndef run_experiment(experiment_name, learning_rate, batch_size, num_epochs, lora_r, lora_alpha, train_data, val_data):\n    \"\"\"Run a single fine-tuning experiment and return results.\"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"  EXPERIMENT: {experiment_name}\")\n    print(f\"  LR={learning_rate}, Batch={batch_size}, Epochs={num_epochs}, LoRA r={lora_r}, alpha={lora_alpha}\")\n    print(f\"{'=' * 60}\\n\")\n\n    # Check if adapter already exists (skip if re-running)\n    adapter_path = os.path.join(SAVE_DIR, experiment_name)\n    if os.path.exists(os.path.join(adapter_path, \"adapter_config.json\")):\n        print(f\"Adapter exists at {adapter_path}. Skipping training.\")\n        results_path = os.path.join(adapter_path, \"results.json\")\n        if os.path.exists(results_path):\n            import json as json_lib\n            with open(results_path, \"r\") as f:\n                return json_lib.load(f)\n        return None\n\n    start_time = time.time()\n    torch.cuda.reset_peak_memory_stats()\n\n    # Load fresh base model and prepare for LoRA\n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n    model = prepare_model_for_kbit_training(model)\n\n    lora_config = LoraConfig(\n        r=lora_r, lora_alpha=lora_alpha, lora_dropout=0.05,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        bias=\"none\", task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, lora_config)\n\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\\n\")\n\n    # Tokenize datasets\n    def tokenize_fn(examples):\n        tokenized = tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n        return tokenized\n\n    tokenized_train = train_data.map(tokenize_fn, batched=True, remove_columns=train_data.column_names)\n    tokenized_val = val_data.map(tokenize_fn, batched=True, remove_columns=val_data.column_names)\n\n    gradient_accumulation = max(1, 8 // batch_size)\n    output_dir = f\"./results/{experiment_name}\"\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=gradient_accumulation,\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        logging_steps=25,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        fp16=True,\n        report_to=\"none\",\n        optim=\"paged_adamw_8bit\",\n        warmup_ratio=0.1,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        args=training_args,\n        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    )\n\n    train_result = trainer.train()\n\n    training_time = time.time() - start_time\n    peak_memory = torch.cuda.max_memory_allocated() / 1e9\n    eval_results = trainer.evaluate()\n\n    # Save adapter\n    model.save_pretrained(adapter_path)\n    tokenizer.save_pretrained(adapter_path)\n\n    results = {\n        \"experiment\": experiment_name,\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"gradient_accumulation\": gradient_accumulation,\n        \"epochs\": num_epochs,\n        \"lora_r\": lora_r,\n        \"lora_alpha\": lora_alpha,\n        \"train_loss\": round(train_result.training_loss, 4),\n        \"eval_loss\": round(eval_results[\"eval_loss\"], 4),\n        \"training_time_min\": round(training_time / 60, 1),\n        \"peak_gpu_memory_gb\": round(peak_memory, 2),\n    }\n\n    import json as json_lib\n    with open(os.path.join(adapter_path, \"results.json\"), \"w\") as f:\n        json_lib.dump(results, f, indent=2)\n\n    print(f\"\\n--- Results: Train Loss={results['train_loss']:.4f}, Eval Loss={results['eval_loss']:.4f}, \"\n          f\"Time={results['training_time_min']:.1f}min, GPU={results['peak_gpu_memory_gb']:.2f}GB ---\")\n\n    del model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    return results",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 1: LR=2e-4, batch=4, epochs=3, LoRA r=16\n",
    "exp1_results = run_experiment(\n",
    "    \"exp1_lr2e4_bs4_ep3_r16\", learning_rate=2e-4, batch_size=4,\n",
    "    num_epochs=3, lora_r=16, lora_alpha=32,\n",
    "    train_data=train_dataset, val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 2: LR=5e-5, batch=2, epochs=3, LoRA r=16\n",
    "exp2_results = run_experiment(\n",
    "    \"exp2_lr5e5_bs2_ep3_r16\", learning_rate=5e-5, batch_size=2,\n",
    "    num_epochs=3, lora_r=16, lora_alpha=32,\n",
    "    train_data=train_dataset, val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment 3: LR=1e-4, batch=4, epochs=2, LoRA r=8\n",
    "exp3_results = run_experiment(\n",
    "    \"exp3_lr1e4_bs4_ep2_r8\", learning_rate=1e-4, batch_size=4,\n",
    "    num_epochs=2, lora_r=8, lora_alpha=16,\n",
    "    train_data=train_dataset, val_data=val_dataset,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Experiment Comparison Table\n",
    "experiment_results = [r for r in [exp1_results, exp2_results, exp3_results] if r is not None]\n",
    "comparison_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "display_cols = [\"experiment\", \"learning_rate\", \"batch_size\", \"epochs\",\n",
    "    \"lora_r\", \"lora_alpha\", \"train_loss\", \"eval_loss\", \"training_time_min\", \"peak_gpu_memory_gb\"]\n",
    "comparison_df = comparison_df[display_cols]\n",
    "comparison_df.columns = [\"Experiment\", \"Learning Rate\", \"Batch Size\", \"Epochs\",\n",
    "    \"LoRA Rank\", \"LoRA Alpha\", \"Train Loss\", \"Eval Loss\", \"Time (min)\", \"GPU Mem (GB)\"]\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"EXPERIMENT COMPARISON TABLE\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "best_idx = comparison_df[\"Eval Loss\"].idxmin()\n",
    "best_experiment = experiment_results[best_idx][\"experiment\"]\n",
    "print(f\"\\nBest experiment: {best_experiment} (Eval Loss: {comparison_df.loc[best_idx, 'Eval Loss']:.4f})\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "short_labels = [f\"Exp {i+1}\" for i in range(len(experiment_results))]\n",
    "colors = [\"steelblue\", \"coral\", \"mediumseagreen\"]\n",
    "\n",
    "axes[0].bar(short_labels, comparison_df[\"Train Loss\"], color=colors)\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axes[1].bar(short_labels, comparison_df[\"Eval Loss\"], color=colors)\n",
    "axes[1].set_title(\"Validation Loss\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "\n",
    "axes[2].bar(short_labels, comparison_df[\"Time (min)\"], color=colors)\n",
    "axes[2].set_title(\"Training Time\")\n",
    "axes[2].set_ylabel(\"Minutes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Evaluation & Performance Metrics\n",
    "\n",
    "### Quantitative Metrics\n",
    "1. **BLEU Score**: n-gram overlap between generated and reference answers\n",
    "2. **ROUGE Score**: Recall-oriented overlap (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "3. **Perplexity**: Model confidence on test data (lower = better)\n",
    "\n",
    "### Qualitative Evaluation\n",
    "4. **Base vs. Fine-tuned Comparison**: Side-by-side responses\n",
    "5. **Out-of-domain Testing**: Non-medical query handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the Best Model for Evaluation\n",
    "best_adapter_path = os.path.join(SAVE_DIR, best_experiment)\n",
    "print(f\"Loading best adapter from: {best_adapter_path}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, best_adapter_path)\n",
    "model.eval()\n",
    "print(\"Fine-tuned model loaded successfully.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Response Generation Helper\n",
    "def generate_response(model, tokenizer, question, max_new_tokens=256):\n",
    "    \"\"\"Generate a response from the model given a medical question.\"\"\"\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{question}</s>\\n\"\n",
    "        f\"<|assistant|>\\n\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens, temperature=0.7,\n",
    "            top_p=0.9, do_sample=True, pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Quick test\n",
    "test_q = \"What are the symptoms of diabetes?\"\n",
    "print(f\"Q: {test_q}\")\n",
    "print(f\"A: {generate_response(model, tokenizer, test_q)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute BLEU & ROUGE on Test Set\n",
    "NUM_EVAL = min(100, len(test_dataset))\n",
    "eval_subset = test_dataset.select(range(NUM_EVAL))\n",
    "\n",
    "print(f\"Evaluating on {NUM_EVAL} test examples...\")\n",
    "print(\"Generating responses (this may take a few minutes)...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, example in enumerate(eval_subset):\n",
    "    pred = generate_response(model, tokenizer, example[\"question\"], max_new_tokens=200)\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"answer\"])\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Generated {i + 1}/{NUM_EVAL} responses...\")\n",
    "\n",
    "print(f\"  Done! Generated {NUM_EVAL} responses.\")\n",
    "\n",
    "# BLEU Score\n",
    "smoothie = SmoothingFunction().method1\n",
    "bleu_scores = []\n",
    "for ref, pred in zip(references, predictions):\n",
    "    ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "    pred_tokens = nltk.word_tokenize(pred.lower())\n",
    "    if len(pred_tokens) == 0:\n",
    "        bleu_scores.append(0.0)\n",
    "        continue\n",
    "    score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\"\\nBLEU Score: {avg_bleu:.4f}\")\n",
    "\n",
    "# ROUGE Score\n",
    "rouge = hf_evaluate.load(\"rouge\")\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "print(f\"ROUGE-1:   {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2:   {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L:   {rouge_results['rougeL']:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute Perplexity on Test Set\n",
    "print(\"Computing perplexity on test set...\")\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "for i, example in enumerate(eval_subset):\n",
    "    inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"input_ids\"])\n",
    "    num_tokens = inputs[\"attention_mask\"].sum().item()\n",
    "    total_loss += outputs.loss.item() * num_tokens\n",
    "    total_tokens += num_tokens\n",
    "\n",
    "avg_loss = total_loss / total_tokens\n",
    "perplexity = float(np.exp(avg_loss))\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity:   {perplexity:.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Summary Table\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"BLEU Score\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"Perplexity\"],\n",
    "    \"Value\": [f\"{avg_bleu:.4f}\", f\"{rouge_results['rouge1']:.4f}\", f\"{rouge_results['rouge2']:.4f}\",\n",
    "              f\"{rouge_results['rougeL']:.4f}\", f\"{perplexity:.2f}\"],\n",
    "    \"Interpretation\": [\n",
    "        \"Higher is better (n-gram precision)\", \"Higher is better (unigram recall)\",\n",
    "        \"Higher is better (bigram recall)\", \"Higher is better (longest common subsequence)\",\n",
    "        \"Lower is better (model confidence)\"],\n",
    "})\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_df.to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Base vs. Fine-Tuned Model Comparison\n",
    "comparison_questions = [\n",
    "    \"What are the main symptoms of Type 2 diabetes?\",\n",
    "    \"Explain the mechanism of action of aspirin.\",\n",
    "    \"What is the difference between benign and malignant tumors?\",\n",
    "    \"What are the risk factors for cardiovascular disease?\",\n",
    "    \"Describe the function of the liver in the human body.\",\n",
    "    \"What is hypertension and how is it treated?\",\n",
    "    \"What are the stages of wound healing?\",\n",
    "    \"Explain what an ECG measures and why it is useful.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASE MODEL vs. FINE-TUNED MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, question in enumerate(comparison_questions):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "\n",
    "    finetuned_response = generate_response(model, tokenizer, question)\n",
    "    with model.disable_adapter():\n",
    "        base_response = generate_response(model, tokenizer, question)\n",
    "\n",
    "    print(f\"\\n  BASE MODEL:\\n  {base_response[:300]}\")\n",
    "    print(f\"\\n  FINE-TUNED MODEL:\\n  {finetuned_response[:300]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Out-of-Domain Testing\n",
    "print(\"=\" * 70)\n",
    "print(\"OUT-OF-DOMAIN QUERY TESTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ood_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I write a Python for loop?\",\n",
    "    \"What is the recipe for chocolate cake?\",\n",
    "]\n",
    "\n",
    "for question in ood_questions:\n",
    "    response = generate_response(model, tokenizer, question, max_new_tokens=150)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response[:250]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Interactive UI with Gradio\n",
    "\n",
    "Gradio chat interface for interacting with the fine-tuned medical assistant. Uses `share=True` for Colab compatibility."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Gradio Chat Interface\n",
    "import gradio as gr\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"Generate a response from the fine-tuned medical assistant.\"\"\"\n",
    "    if not message.strip():\n",
    "        return \"Please enter a medical question.\"\n",
    "    response = generate_response(model, tokenizer, message, max_new_tokens=300)\n",
    "    if not response:\n",
    "        return \"Could not generate a response. Please try rephrasing.\"\n",
    "    return response\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Medical Assistant - Fine-Tuned TinyLlama\",\n",
    "    description=\"Ask medical questions and receive AI-generated answers. Fine-tuned on medical flashcard data using LoRA.\",\n",
    "    examples=[\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"Explain the mechanism of action of aspirin.\",\n",
    "        \"What is the difference between Type 1 and Type 2 diabetes?\",\n",
    "        \"What are the risk factors for cardiovascular disease?\",\n",
    "        \"Describe the stages of wound healing.\",\n",
    "    ],\n",
    "    theme=\"soft\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True, debug=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Conclusion\n",
    "\n",
    "### Summary\n",
    "1. **Dataset**: Preprocessed 3,000 medical Q&A pairs from `medalpaca/medical_meadow_medical_flashcards`\n",
    "2. **Fine-Tuning**: Used 4-bit quantization + LoRA on TinyLlama-1.1B-Chat, training ~0.5% of parameters\n",
    "3. **Experimentation**: Compared 3 hyperparameter configurations tracking loss, GPU memory, and training time\n",
    "4. **Evaluation**: Measured BLEU, ROUGE, perplexity + qualitative base vs. fine-tuned comparison\n",
    "5. **Deployment**: Gradio chat interface for interactive medical Q&A\n",
    "\n",
    "### Key Findings\n",
    "- Fine-tuning significantly improves domain-specific response quality vs. the base model\n",
    "- LoRA is highly effective for domain adaptation with minimal parameter overhead\n",
    "- 4-bit quantization enables training on consumer-grade GPUs\n",
    "\n",
    "### Limitations\n",
    "- TinyLlama (1.1B params) has inherent reasoning limitations vs. larger models\n",
    "- Medical flashcards cover broad topics but may lack depth in specialized areas\n",
    "\n",
    "### Future Work\n",
    "- Fine-tune on larger datasets (PubMedQA, MedMCQA)\n",
    "- Experiment with larger models (Gemma 2B, Llama 3.2 3B)\n",
    "- Implement retrieval-augmented generation (RAG) for source-grounded answers"
   ]
  }
 ]
}